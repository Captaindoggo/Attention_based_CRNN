{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "721451d4414547d59c2b2398e82a4836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fb995f61d39470dacc345fc092fceaf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4d83a4fa19de47f5b9420a9385f21dae",
              "IPY_MODEL_6e3d1cd732a546e1b47c3e71b9a1780c"
            ]
          }
        },
        "8fb995f61d39470dacc345fc092fceaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d83a4fa19de47f5b9420a9385f21dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8c17514edeb4450819177bb2b8b3a41",
            "_dom_classes": [],
            "description": " 24%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 8080,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6029df1b06914199a4bd89c37002b767"
          }
        },
        "6e3d1cd732a546e1b47c3e71b9a1780c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_858f0d0769e94cc5bde6238362345f84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1972/8080 [1:00:58&lt;2:30:23,  1.48s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50aa904b90e74d07b407d13b0522400c"
          }
        },
        "e8c17514edeb4450819177bb2b8b3a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6029df1b06914199a4bd89c37002b767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "858f0d0769e94cc5bde6238362345f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50aa904b90e74d07b407d13b0522400c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuzaXdkytumz"
      },
      "source": [
        "# Homework №3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57thqlsotum8"
      },
      "source": [
        "This homework will be dedicated to Keyword Spotting (KWS), streaming and speedup NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feqcq6lXtum8"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b218wsnftum8",
        "outputId": "8f8ac5c0-b23e-4c0b-d0ca-d77d01a9e5bd"
      },
      "source": [
        "import os\n",
        "datadir = \"speech_commands\"\n",
        "\n",
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
        "!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
        "\n",
        "samples_by_target = {\n",
        "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
        "    for cls in os.listdir(datadir)\n",
        "    if os.path.isdir(os.path.join(datadir, cls))\n",
        "}\n",
        "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-16 08:58:45--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.2.112, 2607:f8b0:4004:805::2010\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.2.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G   239MB/s    in 7.1s    \n",
            "\n",
            "2020-12-16 08:58:52 (201 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n",
            "Classes: bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, nine, no, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYpYEULPtum9"
      },
      "source": [
        "    Choose from 1 to 3 keywords to your liking, and use the rest as negative examples.\n",
        "    We recommend to use sheila and/or marvin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1DX-LOtum9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVFeb2nStum-"
      },
      "source": [
        "    In this homework assignment, you will need to implement a model for finding a keyword in a stream.\n",
        "\n",
        "        1) https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf\n",
        "            It is recommended to implement the version with CNN because it is easy and improves the model.\n",
        "\n",
        "        2) What about streaming?\n",
        "           This model works differently during training and inferance.\n",
        "           During training you have some fixed input and you know that it has a keyword (or not).\n",
        "           During the inferance, you read the T frames and make a prediction on them. And the next step is to read the T+1 frame,\n",
        "           run the neural network just for it, and make a prediction based on it and the T-1 of the previous frames.\n",
        "           This way you don't make unnecessary calculations.\n",
        "\n",
        "           So, your model should support streaming mode.\n",
        "           To demonstrate the work in streaming mode, take two random audio tracks of 10-20 seconds and glue them together\n",
        "           so that your keyword will be between them. Run the model through this glued track and draw how the probability of your keyword changing over time.\n",
        "\n",
        "        3) A good KWS is a robust KWS, so we ask you to implement as many augmentations as possible.\n",
        "           (bonus) Download any noise from YouTube and add it as a background noise to the positive data. This helps a lot in real life.\n",
        "           P.S. Use https://www.youtube-dl.org/\n",
        "\n",
        "        4) (bonus) Add more attentions and orthogonality regularization. https://arxiv.org/abs/1910.04500\n",
        "        \n",
        "        5) (bonus) Speedup you model! Implement distillation of your model, for example,\n",
        "            train the LSTM with 256 hidden size and distil it into LSTM with 128 hidden size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqZ6QGhttum-"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEEMBqgqtum-"
      },
      "source": [
        "    1) In this homework you are allowed to use pytorch-lighting.\n",
        "\n",
        "    2) Try to write code more structurally and cleanly!\n",
        "    \n",
        "    3) Good logging of experiments save your nerves and time, so we ask you to use W&B. Log loss, FA/FR rate or something else.\n",
        "        Do not remove the logs until we have checked your work and given you a grade!\n",
        "    \n",
        "    4) (Bonus) We also ask you to organize your code in github repo with Docker and setup.py. You can use my template https://github.com/markovka17/dl-start-pack.\n",
        "    \n",
        "    5) Your work must be reproducable, so fix seed, save the weights of model, and etc.\n",
        "    \n",
        "    6) In the end of your work write inference utils. Anyone should be able to take your weight, load it into the model and run it on some audio track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3KUFzwb53DG",
        "outputId": "51a6e511-a4a0-4533-e3f4-447b860e7924"
      },
      "source": [
        "!pip3 install torchaudio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/f9/618434cf4e46dc975871e1516f5499abef6564ab4366f9b2321ee536be14/torchaudio-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 14.0MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchaudio) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed torch-1.7.1 torchaudio-0.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw18-cJ3yZ_O",
        "outputId": "6bb0b613-5d0a-4766-9b44-535821e35634"
      },
      "source": [
        "!pip3 install --upgrade wandb"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5e/9df94df3bfee51b92b54a5e6fa277d6e1fcdf1f27b1872214b98f55ec0f7/wandb-0.10.12-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 14.0MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 56.1MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/76/39d123d37908a772b6a281d85fbb4384d9db7e13d19d10ad409006bd2962/watchdog-1.0.1.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-1.0.1-cp36-none-any.whl size=72206 sha256=7c11d04ea2211cfbea96f458de80eaecc9792ec3594bb5ddb4330060d934799a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/ce/d8/31a48288b5728794feda5ac479fa324cc1cde4398c29eff064\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=6254683342b5d464414df58cfcdceba2c72aabc823f3cd30c45f758b9d137a9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built watchdog subprocess32\n",
            "Installing collected packages: smmap, gitdb, GitPython, watchdog, docker-pycreds, configparser, subprocess32, sentry-sdk, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.11 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 sentry-sdk-0.19.5 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.10.12 watchdog-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-7zRGfu8j6"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "from torchaudio.transforms import FrequencyMasking, TimeMasking, MelSpectrogram\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import WeightedRandomSampler"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhwF0TzmC_KU"
      },
      "source": [
        "def set_seed(n):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed = n\n",
        "    random.seed(n)\n",
        "    np.random.seed(n)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbsG5xOJxIyQ",
        "outputId": "58e9ae23-8022-408c-d955-6d2122769a0b"
      },
      "source": [
        "names = []\n",
        "words = []\n",
        "lbls = []\n",
        "noises = []\n",
        "for wv in tqdm(samples_by_target.keys()):\n",
        "    if wv != '_background_noise_':\n",
        "        for sample in samples_by_target[wv]:\n",
        "            word = sample.split('/')[1]\n",
        "            if word == 'sheila':\n",
        "                lbl = 1\n",
        "            else:\n",
        "                lbl = 0\n",
        "            names.append(sample)\n",
        "            words.append(word)\n",
        "            lbls.append(lbl)\n",
        "    else:\n",
        "        for noise in samples_by_target[wv]:\n",
        "            if 'README' not in noise:\n",
        "              noises.append(noise)\n",
        "names = np.array(names)\n",
        "lbls = np.array(lbls)\n",
        "words = np.array(words)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 31/31 [00:00<00:00, 488.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQN8jGD0upo7"
      },
      "source": [
        "class CommandsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, wavs, labels, words, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.wavs=wavs\n",
        "        self.labels=labels\n",
        "        self.words=words\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        wav_file = os.path.join(self.root_dir,\n",
        "                                self.wavs[idx])\n",
        "        \n",
        "        wav = torchaudio.load(wav_file)[0][0]\n",
        "        lbl = self.labels[idx]\n",
        "        word = self.words[idx]\n",
        "        \n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        sample = [wav, lbl, word]\n",
        "        return sample\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRFBAIt2xkyW"
      },
      "source": [
        "class LogMelSpectrogram(nn.Module):\n",
        "\n",
        "    def __init__(self, sample_rate: int = 16000, n_mels: int = 40, masking=True):\n",
        "        super(LogMelSpectrogram, self).__init__()\n",
        "        self.transform = MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels, n_fft=1024, hop_length=256, f_min=0, f_max=8000)\n",
        "        self.masking=masking\n",
        "        if masking:\n",
        "          self.freq_masking = FrequencyMasking(freq_mask_param=10)\n",
        "          self.time_masking = TimeMasking(time_mask_param=30)\n",
        "\n",
        "\n",
        "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        spectrogram = self.transform(waveform).squeeze()\n",
        "        if self.masking:\n",
        "          spectrogram = self.freq_masking(spectrogram)\n",
        "          spectrogram = self.time_masking(spectrogram)\n",
        "\n",
        "        return torch.log(spectrogram + 1e-9)\n",
        "\n",
        "\n",
        "class Noiser(object):\n",
        "    def __init__(self, noises_list, alpha_low=0.0001, alpha_high=0.0003, p=1.0):\n",
        "        self.a_low = alpha_low\n",
        "        self.a_high=alpha_high\n",
        "        self.noises_list = noises_list\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        noise_name = self.noises_list[np.random.randint(low=0, high=len(self.noises_list))]\n",
        "        noise = torchaudio.load(noise_name)[0][0]\n",
        "        l = np.random.randint(low=0, high=len(noise)-len(wav)-1)\n",
        "        r = l+len(wav)\n",
        "        noise = noise[l:r]\n",
        "        alpha = np.random.uniform(low=0.0001, high=0.0003)\n",
        "        wav = wav + wav*alpha\n",
        "        wav.clamp_(-1, 1)\n",
        "        return wav\n",
        "\n",
        "def my_collate(data):\n",
        "    wav_batch=[]\n",
        "    lbls = []\n",
        "    for sample in data:\n",
        "      wav_batch.append(sample[0])\n",
        "      lbls.append(sample[1])\n",
        "    wav_batch=pad_sequence(wav_batch, batch_first=True)\n",
        "    lbls=torch.tensor(lbls)\n",
        "    return wav_batch, lbls\n",
        "\n",
        "def my_sampler(target):\n",
        "    class_sample_count = np.array([len(np.where(target == t)[0]) for t in np.unique(target)])\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.double()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler\n",
        "\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P0pIdcIGSvC"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, inn, linn, n_classes):\n",
        "        super(Attention, self).__init__()\n",
        "        self.Wxb = nn.Linear(inn, inn)\n",
        "        self.Vt = nn.Linear(inn, 1, bias=False)\n",
        "        self.t = nn.Tanh()\n",
        "\n",
        "        self.out = nn.Linear(linn, n_classes, bias=False)\n",
        "        self.sftmx = nn.Softmax(dim=-1)\n",
        "        self.lsftmx = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        apl_a = []\n",
        "        for x_ in x:\n",
        "          x_ = self.Wxb(x_)\n",
        "          x_ = self.t(x_)\n",
        "          x_ = self.Vt(x_)\n",
        "          apl_a.append(x_)\n",
        "        apl_a = torch.cat(apl_a, dim=1)\n",
        "        x = x.transpose(0, 1)\n",
        "        apl_a = self.sftmx(apl_a).unsqueeze(1)\n",
        "        res = torch.bmm(apl_a, x).squeeze()\n",
        "        res = self.lsftmx(self.out(res))\n",
        "        return res\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75kDhTkc7Viy"
      },
      "source": [
        "class AtnCRNN(nn.Module):\n",
        "    def __init__(self, inn, hidden, num_layers, device, n_classes=2, nd=2):\n",
        "        super(AtnCRNN, self).__init__()\n",
        "        self.conv5 = nn.Conv1d(inn, inn, kernel_size=5, stride=2, dilation=1, groups=inn)\n",
        "        self.conv20 = nn.Conv1d(inn, hidden, kernel_size=1, stride=8, groups=int(inn/20))\n",
        "\n",
        "        self.rnn = nn.GRU(hidden, hidden, num_layers=num_layers, dropout=0.1, bidirectional=True)\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden=hidden\n",
        "        self.device=device\n",
        "        self.attention = Attention(hidden*2, hidden*nd, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = torch.zeros(self.num_layers*2, x.shape[0], self.hidden).to(self.device)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv20(x)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x, hidn = self.rnn(x, h)\n",
        "        x = self.attention(x)\n",
        "        return x"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg1oIzVcxD0U"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def FA_FR_scores(preds, targets):\n",
        "    FA = sum(preds[targets == 0])/len(targets)\n",
        "    FR = sum(targets[preds == 0])/len(targets)\n",
        "    return FA, FR"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1D_EgqK_-cv"
      },
      "source": [
        "def count_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    return sum([np.prod(p.size()) for p in model_parameters])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFBTu1I_C5wT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09688029-9222-4396-e089-e29384306c9d"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "#wandb.init(project=\"dla-homework-2\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TteKvako_-3I"
      },
      "source": [
        "root_dir='/content'\n",
        "\n",
        "#dataset = CommandsDataset(root_dir, names, lbls, words, transform=Noiser(noises_list=noises))\n",
        "train_len = int(len(lbls)*0.8)\n",
        "\n",
        "idxs = np.array(range(len(lbls)))\n",
        "np.random.shuffle(idxs)\n",
        "\n",
        "train_names = names[idxs[:train_len]]\n",
        "train_lbls = lbls[idxs[:train_len]]\n",
        "train_words = words[idxs[:train_len]]\n",
        "train = CommandsDataset(root_dir, train_names, train_lbls, train_words, transform=Noiser(noises_list=noises))\n",
        "#train, val = torch.utils.data.random_split(dataset, (train_len, val_len))\n",
        "\n",
        "val_names = names[idxs[train_len:]]\n",
        "val_lbls = lbls[idxs[train_len:]]\n",
        "val_words = words[idxs[train_len:]]\n",
        "val = CommandsDataset(root_dir, val_names, val_lbls, val_words, transform=Noiser(noises_list=noises))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpzWH-pgDR6d"
      },
      "source": [
        "train_sampler = my_sampler(train_lbls)\n",
        "val_sampler   = my_sampler(val_lbls)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB4MvtSVA5xj"
      },
      "source": [
        "batch_size=256\n",
        "n_mels=40\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, sampler=train_sampler, collate_fn=my_collate)\n",
        "\n",
        "val_loader = DataLoader(val, batch_size=batch_size, sampler=val_sampler, collate_fn=my_collate)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JwKkSKgCoE9"
      },
      "source": [
        "melspec = LogMelSpectrogram(n_mels=n_mels).to(device)\n",
        "melspec_val = LogMelSpectrogram(n_mels=n_mels, masking=False).to(device)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99tNhdFPPJc"
      },
      "source": [
        "model=AtnCRNN(n_mels, 128, 2, device).to(device)\n",
        "lr=0.001\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "NoFlAml2P_S_",
        "outputId": "a9e81ec7-5278-45dc-d57a-8b2d05bf70be"
      },
      "source": [
        "!wandb login\n",
        "import wandb\n",
        "wandb.init(project=\"dla-hw-3\")\n",
        "wandb.watch(model)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcaptain_doggo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bright-planet-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/captain_doggo/dla-hw-3\" target=\"_blank\">https://wandb.ai/captain_doggo/dla-hw-3</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/captain_doggo/dla-hw-3/runs/3t96qvum\" target=\"_blank\">https://wandb.ai/captain_doggo/dla-hw-3/runs/3t96qvum</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201216_103814-3t96qvum</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7fec1361a4a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "721451d4414547d59c2b2398e82a4836",
            "8fb995f61d39470dacc345fc092fceaf",
            "4d83a4fa19de47f5b9420a9385f21dae",
            "6e3d1cd732a546e1b47c3e71b9a1780c",
            "e8c17514edeb4450819177bb2b8b3a41",
            "6029df1b06914199a4bd89c37002b767",
            "858f0d0769e94cc5bde6238362345f84",
            "50aa904b90e74d07b407d13b0522400c"
          ]
        },
        "id": "qT2Kx20oAOn_",
        "outputId": "5515777a-cfc5-4f7c-f3f1-50e0338e5cee"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "epochs = 40\n",
        "pbar = tqdm_notebook(total = epochs*(len(train)//batch_size))\n",
        "for epoch in range(epochs):\n",
        "  running_loss=0.0\n",
        "  val_loss=0.0\n",
        "  ctr=0\n",
        "  val_ctr=0\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "    X, y = batch\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    X_mel = melspec(X)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred = model(X_mel)\n",
        "    loss = F.nll_loss(pred, y)\n",
        "    running_loss+=loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "    optimizer.step()\n",
        "    ctr+=1\n",
        "    pbar.update(1)\n",
        "  \n",
        "  model.eval()\n",
        "  val_preds = []\n",
        "  true = []\n",
        "  for batch in val_loader:\n",
        "    X, y = batch\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    X_mel = melspec_val(X)\n",
        "    with torch.no_grad():\n",
        "      pred = model(X_mel)\n",
        "      loss = F.nll_loss(pred, y)\n",
        "      val_loss+=loss.item()\n",
        "    pred = (torch.argmax(pred, dim=1)).cpu()\n",
        "    val_preds.extend(pred.tolist())\n",
        "    true.extend(y.cpu().tolist())\n",
        "    val_ctr+=1\n",
        "  val_preds =np.array(val_preds)\n",
        "  true = np.array(true)\n",
        "  FA, FR = FA_FR_scores(val_preds, true)\n",
        "  acc = accuracy_score(true, val_preds)\n",
        "  wandb.log({'train loss':running_loss/ctr, 'val loss':val_loss/val_ctr, 'val FA':FA, 'val FR':FR, 'val acc':acc})\n",
        "  print('train loss', running_loss/ctr, 'val loss', val_loss/val_ctr, 'val FA', FA, 'val FR', FR, 'val acc', acc)\n",
        "\n",
        "\n",
        "pbar.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "721451d4414547d59c2b2398e82a4836",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=8080.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "train loss 0.6054163586917182 val loss 0.3603630451595082 val FA 0.08953263808420239 val FR 0.04990343762070298 val acc 0.8605639242950947\n",
            "train loss 0.5315543764036864 val loss 0.34648051624204595 val FA 0.1164156044804944 val FR 0.020393974507531866 val acc 0.8631904210119737\n",
            "train loss 0.49767975421081034 val loss 0.2462168105092703 val FA 0.052220934723831595 val FR 0.04380069524913094 val acc 0.9039783700270375\n",
            "train loss 0.4600298119883232 val loss 0.2393428610236037 val FA 0.073928157589803 val FR 0.015604480494399381 val acc 0.9104673619157976\n",
            "train loss 0.435218878127084 val loss 0.18007952822189705 val FA 0.03623020471224411 val FR 0.03398995751255311 val acc 0.9297798377752028\n",
            "train loss 0.4244285635466646 val loss 0.1822102627917832 val FA 0.028273464658169177 val FR 0.04325994592506759 val acc 0.9284665894167632\n",
            "train loss 0.4075709159297896 val loss 0.18416715018889485 val FA 0.03885670142912322 val FR 0.026033217458478176 val acc 0.9351100811123986\n",
            "train loss 0.37703204287096787 val loss 0.16833239501597835 val FA 0.027887215140981074 val FR 0.035843955195056 val acc 0.936268829663963\n",
            "train loss 0.36797539174116306 val loss 0.16674598116500705 val FA 0.035766705291618385 val FR 0.028273464658169177 val acc 0.9359598300502124\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
